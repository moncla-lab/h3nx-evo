{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "ad147721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import ast\n",
    "import re\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation, CompoundLocation\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Align import AlignInfo\n",
    "from Bio.Seq import MutableSeq\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d442c2",
   "metadata": {},
   "source": [
    "#### Some universal functions for reading in data from nextstrain builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "9943deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin_virus_config(virus):\n",
    "    \"\"\"\n",
    "    Read in the config file for this virus to get the paths to alignment, metadata files, etc \n",
    "    as well as metadata about the virus such as how many subtypes, \n",
    "    and which genes are receptor-binding or polymerase\n",
    "    \"\"\"\n",
    "    config_json = f'adaptive_evo_config_{virus}.json'\n",
    "    with open(config_json) as json_handle:\n",
    "        configs = json.load(json_handle)\n",
    "        \n",
    "        \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e07b6a",
   "metadata": {},
   "source": [
    "### Get data for this virus, subtype, gene and organize for the adaptation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "99e280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_viruses_nextstrain_build(virus, subtype, gene, window, min_seqs, year_max, year_min):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in the alignment file for the specified virus and subtype, and extract the specified gene sequence. \n",
    "    Divide the alignment into temporal windows depending on the specified `window` argument (in years).\n",
    "    Each temporal window must contain a minimum or `min_seqs` sequences, otherwise it will be excluded as missing data.\n",
    "    The analysis can be limited to a certain time range by supplying `year_max` and/or `year_min`.\n",
    "    \"\"\"\n",
    "    \n",
    "    configs = readin_virus_config(virus)\n",
    "    \n",
    "    #some dengue files are shared between all genotypes of the same serotype\n",
    "    if virus=='dengue':\n",
    "        serotype = subtype.split('_')[0]\n",
    "    else:\n",
    "        serotype = False\n",
    "    \n",
    "    subunit = False\n",
    "    #find if gene is a subunit of surface protein\n",
    "    if subtype==None:\n",
    "        gene_location_key = \"location\"\n",
    "    else:\n",
    "        #dengue reference files and locations are shared among serotypes\n",
    "        if serotype:\n",
    "            gene_location_key = \"location_\"+str(serotype)\n",
    "        else:\n",
    "            gene_location_key = \"location_\"+str(subtype)\n",
    "            \n",
    "    if gene.upper() == configs['receptor_binding']['virus_gene'].upper():\n",
    "        if \"specify_location\" in configs['receptor_binding'].keys():\n",
    "            subunit = True\n",
    "            parent_gene = configs['receptor_binding']['specify_location']['parent_gene']\n",
    "            gene_location_list = ast.literal_eval(configs['receptor_binding']['specify_location'][gene_location_key])\n",
    "    elif gene.upper() == configs['membrane_fusion']['virus_gene'].upper():\n",
    "        if \"specify_location\" in configs['membrane_fusion'].keys():\n",
    "            subunit = True\n",
    "            parent_gene = configs['membrane_fusion']['specify_location']['parent_gene']\n",
    "            gene_location_list = ast.literal_eval(configs['membrane_fusion']['specify_location'][gene_location_key])\n",
    "        \n",
    "    #Find reference, alignment and meta files (some sub-genic regions may use files from a gene or a whole genome)\n",
    "    if subunit:\n",
    "        alignment_file = configs['alignment_file'].format(virus=virus, subtype=subtype, gene=parent_gene)\n",
    "        meta_file = configs['meta_file'].format(virus=virus, subtype=subtype, gene=parent_gene)\n",
    "        #some are comma-separated, some are tab-separated\n",
    "        metafile_sep = configs['metafile_sep']\n",
    "    else:\n",
    "        alignment_file = configs['alignment_file'].format(virus=virus, subtype=subtype, gene=gene)\n",
    "        meta_file = configs['meta_file'].format(virus=virus, subtype=subtype, gene=gene)\n",
    "        metafile_sep = configs['metafile_sep']\n",
    "        #dengue reference files are shared among serotypes\n",
    "        if serotype:\n",
    "            reference_file = configs['reference_file'].format(virus=virus, subtype=serotype, gene=gene)\n",
    "        else:\n",
    "            reference_file = configs['reference_file'].format(virus=virus, subtype=subtype, gene=gene)\n",
    "    \n",
    "    print(alignment_file)\n",
    "    \n",
    "    # if a minimum year to use is supplied in config, limit seqs to this\n",
    "    if 'min_year' in configs.keys():\n",
    "        year_min = int(configs['min_year'])\n",
    "    \n",
    "    #Find gene location, if domain is sub-genic or reference file contains multiple genes\n",
    "    gene_location = False\n",
    "    #If domain is sub-genic, fetch its position (within genome or parent gene) from config file\n",
    "\n",
    "    if subunit:\n",
    "        #Need to deal with domains the are not contiguous\n",
    "        if len(gene_location_list)==1:\n",
    "            gene_location = SeqFeature(FeatureLocation(gene_location_list[0][0], gene_location_list[0][1]))\n",
    "        else:\n",
    "            compound_locations = []\n",
    "            for location in gene_location_list:\n",
    "                compound_locations.append(FeatureLocation(location[0], location[1]))\n",
    "            gene_location = CompoundLocation(compound_locations)\n",
    "\n",
    "    #for reference files where the gene is named differently\n",
    "    #like for fluC, where the segment is NS, but it contains NS1 and NS2\n",
    "    elif 'specify_locations' in configs.keys() and gene in configs['specify_locations'].keys():       \n",
    "        gene_location_list = ast.literal_eval(configs['specify_locations'][gene])\n",
    "        gene_location = SeqFeature(FeatureLocation(gene_location_list[0][0], gene_location_list[0][1]))\n",
    "\n",
    "    #Find gene location from reference files\n",
    "    else:\n",
    "        for seq_record in SeqIO.parse(reference_file, \"genbank\"):\n",
    "            for feature in seq_record.features:\n",
    "                if feature.type == 'CDS':\n",
    "                    if 'gene' in feature.qualifiers.keys():\n",
    "                        if feature.qualifiers['gene'][0].lower() == gene.lower():\n",
    "                            gene_location = feature.location\n",
    "                    if gene_location==False:\n",
    "                        if 'product' in feature.qualifiers.keys():\n",
    "                            if feature.qualifiers['product'][0].lower() == gene.lower():\n",
    "                                gene_location = feature.location\n",
    "                    if gene_location == False:\n",
    "                        if 'locus_tag' in feature.qualifiers.keys():\n",
    "                            if feature.qualifiers['locus_tag'][0].lower() == gene.lower():\n",
    "                                gene_location = feature.location  \n",
    "                        \n",
    "\n",
    "    #Subset data based on time windows\n",
    "    meta = pd.read_csv(meta_file, sep = metafile_sep)\n",
    "    #drop incomplete date data\n",
    "    meta.drop(meta[meta['date']=='?'].index, inplace=True)\n",
    "    meta = meta[meta[\"date\"].str.contains(\"20XX\")==False]\n",
    "    meta = meta[meta[\"date\"].str.contains(\"36-0\")==False]\n",
    "    meta.drop(meta[meta['date'].str[:4]=='XXXX'].index, inplace=True)\n",
    "    meta.dropna(subset=['date'], inplace=True)\n",
    "    meta['year'] = meta['date'].str[:4].astype('int')\n",
    "    if year_max:\n",
    "        meta.drop(meta[meta['year']>year_max].index, inplace=True)\n",
    "    if year_min:\n",
    "        meta.drop(meta[meta['year']<year_min].index, inplace=True)\n",
    "    \n",
    "    date_range = meta['year'].max() - meta['year'].min()\n",
    "    #Remove egg- and cell-passaged strains\n",
    "    meta.drop(meta[meta['strain'].str[-4:]=='-egg'].index, inplace=True)\n",
    "    meta.drop(meta[meta['strain'].str[-5:]=='-cell'].index, inplace=True)\n",
    "    \n",
    "    #Limit meta data to only strains in alignment file\n",
    "    aligned_isolates = []\n",
    "    with open(alignment_file, \"r\") as aligned_handle:\n",
    "        for isolate in SeqIO.parse(aligned_handle, \"fasta\"):\n",
    "            aligned_isolates.append(isolate.id)\n",
    "    aligned_isolates_df = pd.DataFrame(aligned_isolates, columns=['strain'])\n",
    "    meta = meta.merge(aligned_isolates_df, on='strain', how='inner')\n",
    "    \n",
    "    \n",
    "    #Group viruses by time windows\n",
    "    virus_time_subset = {}\n",
    "    if window == 'all':\n",
    "        years = str(meta['year'].min()) + '-' + str(meta['year'].max())\n",
    "        virus_time_subset[years] = meta['strain'].tolist()\n",
    "    else:\n",
    "        date_window_start = meta['year'].min()\n",
    "        date_window_end = meta['year'].min() + window\n",
    "        while date_window_end <= meta['year'].max():\n",
    "            years = str(date_window_start) + '-' + str(date_window_end)\n",
    "            strains = meta[(meta['year']>=date_window_start) & (meta['year']<date_window_end)]['strain'].tolist()\n",
    "            virus_time_subset[years] = strains\n",
    "            \n",
    "            \n",
    "            #sliding window\n",
    "            date_window_end += 1\n",
    "            date_window_start += 1 \n",
    "    \n",
    "\n",
    "    #Only use time points with enough data:\n",
    "    virus_time_subset = {k:v for k,v in virus_time_subset.items() if len(v)>=min_seqs}\n",
    "        \n",
    "    year_windows = []\n",
    "    seqs_in_window = []\n",
    "    \n",
    "    #Find outgroup sequence from strains at first time point(to make consensus from)\n",
    "    first_window = True\n",
    "    first_window_strains = []\n",
    "    first_window_sequences = []\n",
    "    \n",
    "    alignment_time_subset = {}\n",
    "\n",
    "    \n",
    "    for years, subset_viruses in virus_time_subset.items():\n",
    "\n",
    "        year_windows.append(years)\n",
    "        seqs_in_window.append(len(subset_viruses))\n",
    "        alignment_time_subset[years] = []\n",
    "\n",
    "        #make consensus sequence at first time point\n",
    "        if first_window == True:\n",
    "            first_window_strains+=subset_viruses\n",
    "            first_window = False\n",
    "        \n",
    "\n",
    "        with open(alignment_file, \"r\") as aligned_handle:\n",
    "            for isolate in SeqIO.parse(aligned_handle, \"fasta\"):\n",
    "                if isolate.id in first_window_strains:\n",
    "                    if gene_location:\n",
    "                        gene_record = SeqRecord(seq = gene_location.extract(isolate.seq), \n",
    "                                                id = isolate.id, description = gene)\n",
    "                    else:\n",
    "                        gene_record = SeqRecord(seq = isolate.seq, \n",
    "                                                id = isolate.id, description = gene)\n",
    "                    first_window_sequences.append(gene_record)\n",
    "                if isolate.id in subset_viruses:\n",
    "                    if gene_location:\n",
    "                        alignment_time_subset[years].append(gene_location.extract(isolate.seq))\n",
    "                    else:\n",
    "                        alignment_time_subset[years].append(isolate.seq)\n",
    "\n",
    "    first_window_alignment = MultipleSeqAlignment(first_window_sequences)\n",
    "    if virus=='rsv':\n",
    "        outgroup_seq = AlignInfo.SummaryInfo(first_window_alignment).gap_consensus(ambiguous ='N')\n",
    "    else:\n",
    "        outgroup_seq = AlignInfo.SummaryInfo(first_window_alignment).dumb_consensus(ambiguous ='N')\n",
    "    \n",
    "    has_dup = find_duplication(outgroup_seq)\n",
    "    \n",
    "    #if virus has duplication, want to run Bhatt on entire alignment excluding dup, \n",
    "    #and then separately on the duplicated sequence to look at evolution occurring on top of it\n",
    "    \n",
    "    if has_dup:\n",
    "        outgroup_seq, outgroup_seq_aa, alignment_time_subset = adjust_for_duplications(outgroup_seq, alignment_time_subset)\n",
    "    else:\n",
    "        outgroup_seq_aa = outgroup_seq.translate()\n",
    "\n",
    "        \n",
    "    return alignment_time_subset, outgroup_seq, year_windows \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "40958550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplication(outgroup_seq):\n",
    "    \"\"\"\n",
    "    Duplication events (or any insertions) will be signified in the outgroup sequence \n",
    "    by a series of consecutive --- placeholders. Find if there is a duplication in this \n",
    "    evolution of this virus.\n",
    "    \"\"\"\n",
    "    has_dup = False\n",
    "    outgroup_seq_str = str(outgroup_seq)\n",
    "    #if there are ---s in the outgroup_seq, find where they are\n",
    "    #say that insertion/duplication has to be at least 3 codons long\n",
    "    if re.search(\"-{9,}\", outgroup_seq_str):\n",
    "        has_dup=True\n",
    "        \n",
    "        \n",
    "    return has_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "f5553e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_duplications(outgroup_seq, alignment_time_subset):\n",
    "    \"\"\"\n",
    "    Find the position and length of the duplication.\n",
    "    Remove the duplicated region from the outgroup sequence and the every sequence in the alignment.\n",
    "    Evolution on the duplicated region will be considered separately because the outgroup consensus \n",
    "    for this region needs to done from the first timepoint where there are sequences with the duplication\n",
    "    \"\"\"\n",
    "\n",
    "    outgroup_seq_str = str(outgroup_seq)\n",
    "    #find where the duplication is by locating ---s in the outgroup_seq\n",
    "    if re.search(\"-{9,}\", outgroup_seq_str):\n",
    "        dup_start, dup_end = [(x.start(),x.end()) for x in re.finditer(r'-{9,}', outgroup_seq_str)][0]\n",
    "\n",
    "\n",
    "    outgroup_wo_dup = Seq(outgroup_seq_str[:dup_start]+outgroup_seq_str[dup_end:])\n",
    "    outgroup_wo_dup_aa = outgroup_wo_dup.translate()\n",
    "\n",
    "    # remove the duplicated portion from the main alignment\n",
    "    alignment_time_subset_wo_dup = {}\n",
    "    for dates, strain_seqs in alignment_time_subset.items():\n",
    "        strain_seqs_wo_dup = [Seq(str(x)[:dup_start]+str(x)[dup_end:]) for x in strain_seqs]\n",
    "        alignment_time_subset_wo_dup[dates] = strain_seqs_wo_dup\n",
    "    \n",
    "        \n",
    "    \n",
    "    return outgroup_wo_dup, outgroup_wo_dup_aa, alignment_time_subset_wo_dup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "9ee51e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_alignment_outgroup_into_codons(virus, subtype, gene, window, min_seqs, year_max, year_min):\n",
    "    \"\"\"\n",
    "    Divide the outgroup sequence and each sequence in the alignment into a list of codons.\n",
    "    The outgroup sequence will be returned as a list of lists, ex: [['ATG'],['GAA'],['ATT']]\n",
    "    The alignment will be returned as a dictionary, where the key is the time window and the \n",
    "    value is the alignment of sequences sampled during that time window. \n",
    "    The alignment will be given as a list of lists (each element is the sequence of an isolate, \n",
    "    given as a list of its codons) \n",
    "    \"\"\"\n",
    "    \n",
    "    (alignment_time_subset, \n",
    "     outgroup_seq, year_windows) = subset_viruses_nextstrain_build(virus, subtype, gene, \n",
    "                                                                   window, min_seqs, year_max, year_min)\n",
    "    \n",
    "    \n",
    "    #make list of the codon sequence at each position in the gene\n",
    "    if len(outgroup_seq) %3 !=0:\n",
    "        print(f'{virus} {gene} not divisible by 3, check config files')\n",
    "    outgroup_codons = [[str(outgroup_seq[i:i+3])] for i in range(0, len(outgroup_seq), 3)] \n",
    "    \n",
    "    #alignment_time_subset is a dictionary with key as year range \n",
    "    #and value as list of sequences sampled in that time range\n",
    "    #convert the sequence of each sample to a list of codons\n",
    "    aligned_codons_in_window = {}\n",
    "    for k,v in alignment_time_subset.items():\n",
    "        #initiate list to store all samples from this time window\n",
    "        all_isolates = []\n",
    "        for isolate in v:\n",
    "            isolate_codons = [str(isolate[i:i+3]) for i in range(0, len(isolate), 3)] \n",
    "            all_isolates.append(isolate_codons)\n",
    "            \n",
    "        aligned_codons_in_window[k] = all_isolates\n",
    " \n",
    "    return aligned_codons_in_window, outgroup_codons, year_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "07e20d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_organized_data(host, virus, subtype, gene, window, min_seqs, year_max, year_min):\n",
    "    \"\"\"\n",
    "    Save the alignment and outgroup data for a given virus, subtype, gene, window, min_seqs, year_min, year_max combo.\n",
    "    \"\"\"\n",
    "    (aligned_codons_in_window, outgroup_codons, \n",
    "     year_windows) = split_alignment_outgroup_into_codons(virus, subtype, gene, window, \n",
    "                                                          min_seqs, year_max, year_min)\n",
    "    \n",
    "    save_json = {'year_windows':year_windows, 'outgroup_codons':outgroup_codons, \n",
    "                 'aligned_codons_in_window': aligned_codons_in_window}\n",
    "    \n",
    "    \n",
    "    filepath = f'adaptation_results/intermediates/{host}/'\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    save_filename = f'{filepath}input_data_{virus}_{subtype}_{gene}_{window}_{min_seqs}_{year_min}_{year_max}.json'\n",
    "        \n",
    "    with open(save_filename, 'w') as outfile:\n",
    "        json.dump(save_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "e3339aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readin_data(saved_data_name):\n",
    "    with open(saved_data_name) as json_handle:\n",
    "        json_dict = json.load(json_handle)\n",
    "        (aligned_codons_in_window, outgroup_codons, year_windows) = (json_dict['aligned_codons_in_window'], \n",
    "                                                                     json_dict['outgroup_codons'], \n",
    "                                                                     json_dict['year_windows'])\n",
    "    return aligned_codons_in_window, outgroup_codons, year_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "b983ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_outgroup_data(host, virus, subtype, gene, window, min_seqs, year_max, year_min):\n",
    "    \"\"\"\n",
    "    If the alignment and outgroup data has already been organized for this \n",
    "    virus, subtype, gene, window, min_seqs, year_min, year_max combo, then read in the data. \n",
    "    Otherwise, calculate it and save it. Return aligned_codons_in_window, outgroup_codons, year_windows\n",
    "    \"\"\"\n",
    "    \n",
    "    filepath = f'adaptation_results/intermediates/{host}/'\n",
    "    \n",
    "    saved_data_name = f'{filepath}input_data_{virus}_{subtype}_{gene}_{window}_{min_seqs}_{year_min}_{year_max}.json'\n",
    "    \n",
    "    #prepare data and save it if this has not already been done\n",
    "    if not path.exists(saved_data_name):\n",
    "        save_organized_data(host, virus, subtype, gene, window, min_seqs, year_max, year_min)\n",
    "    \n",
    "    #read in the prepared alignment and outgroup data\n",
    "    (aligned_codons_in_window, outgroup_codons, year_windows) = readin_data(saved_data_name)\n",
    "    \n",
    "\n",
    "    return aligned_codons_in_window, outgroup_codons, year_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20499e",
   "metadata": {},
   "source": [
    "### Make bootstrapped dataset (sampled with replacement) for outgroup and alignment codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "13bc98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_alignment(bootstrap_codon_order, isolates):\n",
    "    \"\"\"\n",
    "    For each time point, create a bootstrapped alignment of the same size as the empirical alignment.\n",
    "    The randomized codon order (bootstrap_codon_order) is chosen in `bootstrap_outgroup`.\n",
    "    \"\"\"\n",
    "    \n",
    "    bootstrap_aligned_codons = []\n",
    "    for isolate_codons in isolates:\n",
    "        bootstrap_isolate = [isolate_codons[x] for x in bootstrap_codon_order]\n",
    "        bootstrap_aligned_codons.append(bootstrap_isolate)\n",
    "    \n",
    "    return bootstrap_aligned_codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "f911da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_outgroup(outgroup_codons):\n",
    "    \"\"\"\n",
    "    Sample the empirical outgroup codons with replacement\n",
    "    \"\"\"\n",
    "\n",
    "    bootstrap_codon_order = random.choices(range(len(outgroup_codons)), k=len(outgroup_codons))\n",
    "    bootstrap_outgroup_codons = [outgroup_codons[x] for x in bootstrap_codon_order]\n",
    "    \n",
    "    return bootstrap_outgroup_codons, bootstrap_codon_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "e5fca721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bootstrap_dataset(outgroup_codons, aligned_codons_in_window):\n",
    "    \"\"\"\n",
    "    Make a bootstrap outgroup sequence and alignment, with the same \n",
    "    number of samples per time window as the empirical data \n",
    "    \"\"\"\n",
    "            \n",
    "    bootstrap_outgroup_codons, bootstrap_codon_order = bootstrap_outgroup(outgroup_codons)\n",
    "\n",
    "    \n",
    "    bootstrap_alignment_codons = {}\n",
    "    for years, aligned_isolates in aligned_codons_in_window.items():\n",
    "        bootstrap_aligned_codons = bootstrap_alignment(bootstrap_codon_order, aligned_isolates)\n",
    "        bootstrap_alignment_codons[years] = bootstrap_aligned_codons\n",
    "    \n",
    "        \n",
    "    return bootstrap_outgroup_codons, bootstrap_alignment_codons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8fd1a",
   "metadata": {},
   "source": [
    "### Within each time window, find the number of polymorphisms relative to the outgroup, and the frequency of those polymorphisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "a61ad7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_binning(x, midfreq_high, midfreq_low):\n",
    "    \"\"\"\n",
    "    Given a polymorphism frequency, return a frequency bin (fixed, high-freq, mid-freq, or low-freq)\n",
    "    \"\"\"\n",
    "\n",
    "    #nan frequencies are when there is no sequence coverage at the given position\n",
    "    if math.isnan(x):\n",
    "        f_bin = float('nan')\n",
    "    else:\n",
    "        if x == 1.0:\n",
    "            f_bin = 'f'\n",
    "        elif x>=midfreq_high:\n",
    "            f_bin = 'h'\n",
    "        elif x<midfreq_high and x>=midfreq_low:\n",
    "            f_bin = 'm'\n",
    "        elif x<midfreq_low:\n",
    "            f_bin='l'\n",
    "\n",
    "    return f_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "d4117e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_polymorphic_frequency(outgroup_codon, alignment_codons, midfreq_high, midfreq_low):\n",
    "    \"\"\"\n",
    "    Find the percent of ingroup bases that are polymorphic at each position.\n",
    "    Then convert this to a frequency bin, depending on the midfreq_high and midfreq_low settings\n",
    "    See Bhatt et al, 2011.\n",
    "    \"\"\"\n",
    "    #for each of the 3nts in the codon,\n",
    "    #count the number of polymorphisms\n",
    "    polymorphic_count = [0,0,0]\n",
    "    \n",
    "    \n",
    "    for i in range(3):\n",
    "        outgroup_nt = outgroup_codon[i]\n",
    "        ingroup_nts = [alignment_codons[x][i] for x in range(len(alignment_codons))]\n",
    "        num_polymorphisms = len([x for x in ingroup_nts if x!=outgroup_nt])\n",
    "        polymorphic_count[i] = num_polymorphisms\n",
    "    \n",
    "    polymorphic_frequency = [x/len(alignment_codons) for x in polymorphic_count]\n",
    "    \n",
    "    frequency_bins = [frequency_binning(x, midfreq_high, midfreq_low) for x in polymorphic_frequency]\n",
    "\n",
    "    \n",
    "    return frequency_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "93c8abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_site_type(outgroup, ingroup):\n",
    "    \"\"\"\n",
    "    Determine site type in order to assign a fixation and polymorphism score\n",
    "    \"\"\"\n",
    "\n",
    "    ingroup_bases = set(ingroup)\n",
    "\n",
    "    \n",
    "    if len(ingroup_bases) == 0:\n",
    "        site_type = None\n",
    "    \n",
    "    elif len(ingroup_bases) != 0:\n",
    "        #all ingroup bases are identical\n",
    "        if len(ingroup_bases) == 1:\n",
    "            if outgroup in ingroup_bases:\n",
    "                site_type = 1\n",
    "            elif outgroup not in ingroup_bases:\n",
    "                site_type = 2\n",
    "\n",
    "        #2 different bases in ingroup\n",
    "        elif len(ingroup_bases) == 2:\n",
    "            if outgroup in ingroup_bases:\n",
    "                site_type = 3\n",
    "            elif outgroup not in ingroup_bases:\n",
    "                site_type = 4\n",
    "\n",
    "        #3 different bases in ingroup\n",
    "        elif len(ingroup_bases) == 3:\n",
    "            if outgroup in ingroup_bases:\n",
    "                site_type = 5\n",
    "            elif outgroup not in ingroup_bases:\n",
    "                site_type = 6\n",
    "\n",
    "        #4 different bases in ingroup\n",
    "        elif len(ingroup_bases) == 4:\n",
    "            site_type = 7\n",
    "    \n",
    "    \n",
    "    return site_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "bbc03373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixation_polymorphism_score(outgroup, ingroup):\n",
    "    \"\"\"\n",
    "    Assign a fixation or polymorphism score (see Bhatt et al)\n",
    "    \"\"\"\n",
    "    site_type = determine_site_type(outgroup, ingroup)\n",
    "\n",
    "    \n",
    "    if site_type == None:\n",
    "        Fi = float('nan')\n",
    "        Pi = float('nan')\n",
    "    if site_type == 1:\n",
    "        Fi = 0\n",
    "        Pi = 0\n",
    "    elif site_type == 2:\n",
    "        Fi = 1\n",
    "        Pi = 0\n",
    "    elif site_type in [3,5,7]:\n",
    "        Fi = 0\n",
    "        Pi = 1\n",
    "    elif site_type == 4:\n",
    "        Fi = 0.5\n",
    "        Pi = 0.5\n",
    "    elif site_type == 6:\n",
    "        Fi = (1/3)\n",
    "        Pi = (2/3)\n",
    "    \n",
    "    return Fi, Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "e7234980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_fi_pi(outgroup_codon, alignment_codons):\n",
    "    \"\"\"\n",
    "    Determine the fixation and polymorphism scores at each nucleotide position in codon.\n",
    "    See Bhatt et al, 2010.\n",
    "    \"\"\"\n",
    "    \n",
    "    #for each of the 3nts in the codon,\n",
    "    #store the Fi and Pi score\n",
    "    Fi_score = [0,0,0]\n",
    "    Pi_score = [0,0,0]\n",
    "    \n",
    "    #go through each nt position in the codon to determine polymorphisms relative to the outgroup\n",
    "    for i in range(3):\n",
    "        outgroup_nt = outgroup_codon[i]\n",
    "        ingroup_nts = [alignment_codons[x][i] for x in range(len(alignment_codons))]\n",
    "        #determine fixation and polymorphism by how many different bases appear in the ingroup\n",
    "        Fi, Pi = fixation_polymorphism_score(outgroup_nt, ingroup_nts)\n",
    "        Fi_score[i] = Fi\n",
    "        Pi_score[i] = Pi\n",
    "\n",
    "    return Fi_score, Pi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "ccfc99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_replacement_silent(outgroup_codon, alignment_codons):\n",
    "    \"\"\"\n",
    "    Determine whether a nucleotdie mutation is silent or replacement.\n",
    "    Given the outgroup codon (say AAT), and all codons at this position of the alignment (say AAT, ATT, ATT, ATG),\n",
    "    find the replacement score as the number of ingroup bases that, \n",
    "    if substituted into the outgroup sequence would change the amino acid.\n",
    "    This function takes in codon sequences \n",
    "    and will return the replacement score at each position of the codon (returns a list of 3 replacement scores).\n",
    "    See Bhatt et al, 2010.\n",
    "    \"\"\"\n",
    "    #for each of the 3nts in the codon,\n",
    "    #store the number of ingroup bases that, when subsituted into the outgroup, would change the amino acid\n",
    "    replacement_counts = [0,0,0]\n",
    "\n",
    "    #outgroup amino acid\n",
    "    outgroup_aa = Seq(outgroup_codon).translate()\n",
    "    \n",
    "    #go through each nt position in the codon to determine mutations relative to the outgroup\n",
    "    for i in range(3):\n",
    "        nt_ingroup = [alignment_codons[x][i] for x in range(len(alignment_codons))]\n",
    "        for nt in nt_ingroup:\n",
    "            mutated_outgroup_codon = MutableSeq(outgroup_codon)\n",
    "            mutated_outgroup_codon[i] = nt\n",
    "            mutated_outgroup_aa = Seq(mutated_outgroup_codon).translate()\n",
    "            if outgroup_aa != mutated_outgroup_aa:\n",
    "                replacement_counts[i]+=1\n",
    "\n",
    "    \n",
    "    replacement_scores = [x/len(alignment_codons) for x in replacement_counts]\n",
    "    \n",
    "    return replacement_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "aba513a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_codons(aligned_isolates, outgroup_codons, midfreq_high, midfreq_low):\n",
    "    \"\"\"\n",
    "    For a single time window, find codons where there is a mutation between the aligned sequences and the outgroup.\n",
    "    Calculate replacement and silent scores (see Bhatt et al, 2010), \n",
    "    polymorphic and fixation scores (see Bhatt et al, 2010), and frequency bins (see Bhatt et al, 2011)\n",
    "    \"\"\"\n",
    "    #length of gene in codons\n",
    "    num_codons = len(outgroup_codons)\n",
    "    \n",
    "    #initiate array with one position for each nucleotide in gene to keep track of replacement scores\n",
    "    #and fixation, polymorphism scores\n",
    "    replacement_scores_this_window = [0 for x in range(num_codons*3)] \n",
    "    #default to 0, because, in the absense of mutations to the codon, both Fi and Pi will be 0\n",
    "    fixation_scores_this_window = [0 for x in range(num_codons*3)] \n",
    "    polymorphism_scores_this_window = [0 for x in range(num_codons*3)]   \n",
    "    #also keep track of frequency bins at each site\n",
    "    #as default, sites are \"low\", which could create confusion between sites with 0 mutations \n",
    "    #and those with mutations actually present at low frequencies, like 0.05. \n",
    "    #However, this is ok, because when calculating the Bhatt estimators, the Fi and Pi score will \n",
    "    #be 0 for sites with no mutations, so the estimators will not get contributions from sites with no mutations\n",
    "    frequency_bins_this_window = ['l' for x in range(num_codons*3)]\n",
    "    \n",
    "    #keep track of where mutations have fixed\n",
    "    #dict of fixations has codon position as key and codon sequence as values\n",
    "    fixations_during_this_window = {'syn':{}, 'nonsyn':{}}\n",
    "    \n",
    "    \n",
    "    #walk through each codon in the gene\n",
    "    for i in range(num_codons):\n",
    "    \n",
    "        outgroup = outgroup_codons[i][-1]\n",
    "        #skip ambiguous sequencing\n",
    "        if all(char in ['A','C','G','T'] for char in outgroup):\n",
    "            alignment_codons = [aligned_isolates[x][i] for x in range(len(aligned_isolates))]\n",
    "\n",
    "            #skip ambiguous sequencing\n",
    "            alignment_codons = [x for x in alignment_codons if all(char in ['A','C','G','T'] for char in x)]\n",
    "\n",
    "            #number of unabiguous codons at this position\n",
    "            total_alignment_size = len(alignment_codons)\n",
    "            #number of codons with mutations at this position\n",
    "            #compare to the outgroup sequence, which is either the original outgroup codon seq \n",
    "            #or the most recent fixation at this position, if there has been one\n",
    "            mutations = [x for x in alignment_codons if x!=outgroup]\n",
    "            total_mutations = len(mutations)\n",
    "\n",
    "\n",
    "            #if there are mutations, find whether they are silent or replacement, and update the replacement score\n",
    "            #replacement scores are determined for each nucleotide position\n",
    "            #also find the fixation and polymorphism scores, also determined at each nt pos\n",
    "            if total_mutations > 0:\n",
    "                \n",
    "                #determine replacement scores within this codon\n",
    "                replacement_scores_within_codon = determine_replacement_silent(outgroup, alignment_codons)\n",
    "                #add these replacement scores to the proper nucelotide positions\n",
    "                codon_start = i*3\n",
    "                replacement_scores_this_window[codon_start-1:codon_start+2] = replacement_scores_within_codon\n",
    "                \n",
    "                #find fixation and polymorphism scores within this codon\n",
    "                Fi_score_within_codon, Pi_score_within_codon = determine_fi_pi(outgroup, alignment_codons)\n",
    "                fixation_scores_this_window[codon_start-1:codon_start+2] = Fi_score_within_codon\n",
    "                polymorphism_scores_this_window[codon_start-1:codon_start+2] = Pi_score_within_codon\n",
    "                \n",
    "                #find the polymorphic frequency at each nt position in the codon\n",
    "                freq_bins_within_codon = determine_polymorphic_frequency(outgroup, alignment_codons, midfreq_high, midfreq_low)\n",
    "                frequency_bins_this_window[codon_start-1:codon_start+2] = freq_bins_within_codon\n",
    "                \n",
    "                #if a mutation fixed in this codon, want to update the outgroup sequence\n",
    "                if 'f' in freq_bins_within_codon:\n",
    "                    #apply the fixed nts to the outgroup codon\n",
    "                    fixed_codon = ''.join([alignment_codons[0][i] if freq_bins_within_codon[i]=='f' else outgroup[i] for i in range(3)])\n",
    "                    if Seq(fixed_codon).translate() == Seq(outgroup).translate():\n",
    "                        fixations_during_this_window['syn'][i] = fixed_codon\n",
    "                    else:\n",
    "                        fixations_during_this_window['nonsyn'][i] = fixed_codon\n",
    "\n",
    "    \n",
    "    #Si = 1-Ri\n",
    "    silent_scores_this_window = [1-x for x in replacement_scores_this_window]\n",
    "\n",
    "                \n",
    "    return (replacement_scores_this_window, silent_scores_this_window, \n",
    "            fixation_scores_this_window, polymorphism_scores_this_window, \n",
    "            frequency_bins_this_window, fixations_during_this_window)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "8ff33271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_m_ratio(host, virus, subtype, gene, min_seqs, midfreq_high, midfreq_low, year_max, year_min, bootstrap):\n",
    "    \"\"\"\n",
    "    M = rm/sm\n",
    "    M not expected to vary through time provided that long-term effective population sizes remain sufficiently large\n",
    "    For each gene, calculate M by combining site count among time points (use window='all'). \n",
    "    If the gene is the receptor-binding gene or 'ha_protein', use a non-antigenic gene, \n",
    "    with a sufficient number of neutrally-evolving sites, to calculate M.\n",
    "    \"\"\"\n",
    "    configs = readin_virus_config(virus)\n",
    "    ha_gene = configs['ha_protein']['virus_gene']\n",
    "    receptor_binding_gene = configs['receptor_binding']['virus_gene']\n",
    "    nonantigenic_gene = configs['nonantigenic_gene']\n",
    "\n",
    "    \n",
    "    #if the gene is 'ha_protein' or 'receptor_binding', use a non-antigenic gene \n",
    "    #with a sufficient number of neutrally-evolving site to calculate M\n",
    "    if gene =='ha_protein' or gene =='receptor_binding':\n",
    "        (aligned_codons_in_window, outgroup_codons, \n",
    "         year_windows) = get_alignment_outgroup_data(host, virus, subtype, nonantigenic_gene, \n",
    "                                                     'all', min_seqs, year_max, year_min)\n",
    "    #otherwise, calculate M from this gene    \n",
    "    else:\n",
    "        (aligned_codons_in_window, outgroup_codons, \n",
    "         year_windows) = get_alignment_outgroup_data(host, virus, subtype, gene, \n",
    "                                                     'all', min_seqs, year_max, year_min)\n",
    "        \n",
    "\n",
    "        \n",
    "    #if calculating bootstrapped M, need to first get the bootstrapped data\n",
    "    #then calculate the Ri, Si, Fi, Pi, freq_bins based off bootstrapped data\n",
    "    if bootstrap:\n",
    "        (bootstrap_outgroup_codons, bootstrap_alignment_codons) = make_bootstrap_dataset(outgroup_codons, \n",
    "                                                                                             aligned_codons_in_window)\n",
    "        \n",
    "        \n",
    "        (Ri, Si, Fi, Pi, frequency_bins, fixed_codons) = walk_through_codons(bootstrap_alignment_codons[year_windows[0]], \n",
    "                                                               bootstrap_outgroup_codons, \n",
    "                                                           midfreq_high, midfreq_low)\n",
    "\n",
    "    #otherwise, calc Ri, Si, Fi, Pi, freq_bins from the empirical data\n",
    "    else:\n",
    "        aligned_isolates = aligned_codons_in_window[year_windows[0]]\n",
    "        (Ri, Si, Fi, Pi, frequency_bins, fixed_codons) = walk_through_codons(aligned_isolates, outgroup_codons, \n",
    "                                                           midfreq_high, midfreq_low)\n",
    "        \n",
    "    \n",
    "    #length of the gene the m_ratio is being calculated for\n",
    "    length_nt = len(outgroup_codons)*3\n",
    "\n",
    "    #calculate sm and rm (mid-frequency mutations assumed to be neutrally-evolving)\n",
    "    sm = 0\n",
    "    rm = 0\n",
    "    \n",
    "    #updating outgroup doesn't matter for m_ratio because the window is 'all' years\n",
    "    for i in range(length_nt):\n",
    "        freq_bin = frequency_bins[i]\n",
    "        if freq_bin == 'm':\n",
    "            sm+= (Pi[i]*Si[i])\n",
    "            rm+= (Pi[i]*Ri[i])\n",
    "\n",
    "    #give false, small number if sm=0 to avoid divide by 0 error\n",
    "    if sm ==0:\n",
    "        sm = 0.00000000000000001\n",
    "    m_ratio = rm/sm\n",
    "    \n",
    "    return m_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "1fcb96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bhatt_estimators(aligned_codons_in_window, outgroup_codons, year_windows, m_ratio, \n",
    "                          midfreq_high, midfreq_low, bootstrap, update_outgroup):\n",
    "    \"\"\"\n",
    "    Use Fi, Pi, Ri, Si and frequency bins to calculate the Bhatt estimators \n",
    "    sf, rf, sh, rh, sm, rm, sl, and rl for each time window. \n",
    "    Calculate a from these estimators.\n",
    "    \"\"\"\n",
    "\n",
    "    length_nt = len(outgroup_codons)*3\n",
    "    \n",
    "    #keep track of adaptive subtitutions in each time window\n",
    "    adaptive_substitutions = []\n",
    "    #initiate list to store the window midpoints\n",
    "    window_midpoints = []\n",
    "    \n",
    "    #initiate fixed_codons dict, to be added to as mutations fix\n",
    "    fixed_codons_all_time = {'syn':0, 'nonsyn':0}\n",
    "\n",
    "    \n",
    "    #for each year window, find mutations in the aligned sequences relative to the outgroup\n",
    "    for year_window in year_windows:\n",
    "        #find the window midpoint (this will be the label for the plot)\n",
    "        window_start = int(year_window[0:4])\n",
    "        window_end = int(year_window[-4:])\n",
    "        window_midpoints.append((window_start + window_end)/2)\n",
    "        \n",
    "        #if counting repeat mutations at same codon, need to increase the count for sites that have already fixed\n",
    "        #fixed sites were saved in 'fixed_codons_all_time' during the last year_window\n",
    "        if update_outgroup:\n",
    "            increase_syn_fixations = fixed_codons_all_time['syn']\n",
    "            increase_nonsyn_fixations = fixed_codons_all_time['nonsyn']\n",
    "            \n",
    "        \n",
    "        #list of isolates in this time window, each given as a list of codon sequences\n",
    "        aligned_isolates = aligned_codons_in_window[year_window]\n",
    "        (Ri, Si, Fi, Pi, frequency_bins, fixed_codons) = walk_through_codons(aligned_isolates, outgroup_codons, \n",
    "                                                               midfreq_high, midfreq_low)\n",
    "        #add codons that were fixed in the last window\n",
    "        if update_outgroup:\n",
    "            fixed_codons_all_time['syn']+=len(fixed_codons['syn'])\n",
    "            fixed_codons_all_time['nonsyn']+=len(fixed_codons['nonsyn'])\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "        #bhatt estimators are the sum over all sites in the nucleotide alignment\n",
    "        sf = 0\n",
    "        rf = 0\n",
    "        sh = 0\n",
    "        rh = 0\n",
    "        sm = 0\n",
    "        rm = 0\n",
    "        sl = 0\n",
    "        rl = 0\n",
    "        \n",
    "        \n",
    "        for i in range(length_nt):\n",
    "            freq_bin = frequency_bins[i]\n",
    "            if freq_bin == 'f':\n",
    "                sf+= (Fi[i]*Si[i])\n",
    "                rf+= (Fi[i]*Ri[i])\n",
    "            elif freq_bin == 'h':\n",
    "                sh+= (Pi[i]*Si[i])\n",
    "                rh+= (Pi[i]*Ri[i])\n",
    "            elif freq_bin == 'm':\n",
    "                sm+= (Pi[i]*Si[i])\n",
    "                rm+= (Pi[i]*Ri[i])\n",
    "            elif freq_bin == 'l':\n",
    "                sl+= (Pi[i]*Si[i])\n",
    "                rl+= (Pi[i]*Ri[i]) \n",
    "                \n",
    "        #if counting repeat mutations at same codon, need to add fixations that have already occurred\n",
    "        #fully fixed mutations will add 1 to sf if silent, or 1 to rf if replacement because sf=Fi*Si=1 or rf=Fi*Ri=1\n",
    "        if update_outgroup:\n",
    "            sf+=increase_syn_fixations\n",
    "            rf+=increase_nonsyn_fixations\n",
    "        \n",
    "        #Calculate equation 1: number of nonneutral sites\n",
    "        al = rl - sl*m_ratio\n",
    "        ah = rh - sh*m_ratio\n",
    "        af = rf - sf*m_ratio\n",
    "        \n",
    "        #set negative a values to zero\n",
    "        if al < 0:\n",
    "            al = 0\n",
    "        if ah < 0:\n",
    "            ah = 0\n",
    "        if af < 0:\n",
    "            af = 0\n",
    "\n",
    "        \n",
    "        #Calculate the number and proportion of all fixed or high-freq sites that have undergone adaptive change\n",
    "        number_adaptive_substitutions = af + ah\n",
    "        adaptive_substitutions.append(number_adaptive_substitutions)\n",
    "        \n",
    "        if update_outgroup:\n",
    "            #Update the outgroup sequence for the next time window\n",
    "            for pos, seq in fixed_codons['syn'].items():\n",
    "                outgroup_codons[pos].append(seq)\n",
    "            for pos, seq in fixed_codons['nonsyn'].items():\n",
    "                outgroup_codons[pos].append(seq)\n",
    "\n",
    "\n",
    "    adaptive_substitutions_per_codon = [x/len(outgroup_codons) for x in adaptive_substitutions]\n",
    "    \n",
    "    if len(window_midpoints)!=0:\n",
    "        rate_of_adaptation, intercept, r_value, p_value, std_err = stats.linregress(window_midpoints, adaptive_substitutions_per_codon)\n",
    "    else:\n",
    "        rate_of_adaptation = 0\n",
    "    \n",
    "    \n",
    "    return window_midpoints, adaptive_substitutions, adaptive_substitutions_per_codon, rate_of_adaptation\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "438b84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bhatt_a(host, virus, subtype, gene, window, min_seqs, year_max, year_min,\n",
    "                 midfreq_high, midfreq_low, bootstrap, update_outgroup):\n",
    "    \n",
    "    \"\"\"\n",
    "    Bhatt a (adaptive substitutions), and rate of adaptation are calculated in `calc_bhatt_estimators`. \n",
    "    This is wrapper function to run `calc_bhatt_estimators` on both empirical data and bootstrapped data (if bootstrap=True)  \n",
    "    \"\"\"\n",
    "    \n",
    "    #Get virus subset\n",
    "    (aligned_codons_in_window, outgroup_codons, \n",
    "     year_windows) = get_alignment_outgroup_data(host, virus, subtype, gene, \n",
    "                                                 window, min_seqs, year_max, year_min)\n",
    "    \n",
    "    \n",
    "    m_ratio = calc_m_ratio(host, virus, subtype, gene, min_seqs, midfreq_high, midfreq_low, year_max, year_min, False)\n",
    "\n",
    "    \n",
    "    (window_midpoints, adaptive_substitutions, adaptive_substitutions_per_codon, \n",
    "     rate_of_adaptation) = calc_bhatt_estimators(aligned_codons_in_window, outgroup_codons, \n",
    "                                                 year_windows, m_ratio, midfreq_high, midfreq_low, \n",
    "                                                 bootstrap, update_outgroup)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    n_bootstraps = 100\n",
    "    bootstrap_count = 0\n",
    "    \n",
    "    bootstrap_adaptive_substitutions = []\n",
    "    bootstrap_adaptive_substitutions_per_codon = []\n",
    "    bootstrap_rates_of_adaptation = []\n",
    "    if bootstrap:\n",
    "        while bootstrap_count < n_bootstraps:\n",
    "            if bootstrap_count%10==0:\n",
    "                print(f'{virus} {gene}: {bootstrap_count} bootstraps done')\n",
    "            bootstrap_count+=1\n",
    "            #Get outgroup\n",
    "            (aligned_codons_in_window, outgroup_codons, \n",
    "             year_windows) = get_alignment_outgroup_data(host, virus, subtype, gene, \n",
    "                                                         window, min_seqs, year_max, year_min)\n",
    "            #Get bootstrapped ancestral seq and alignment\n",
    "            (bootstrap_outgroup_codons, bootstrap_alignment_codons) = make_bootstrap_dataset(outgroup_codons, \n",
    "                                                                                            aligned_codons_in_window)\n",
    "            for x in bootstrap_outgroup_codons:\n",
    "                if len(x)!=1:\n",
    "                    print(gene, x)\n",
    "            #calc bootstrapped m_ratio\n",
    "            bootstrap_m_ratio = calc_m_ratio(host, virus, subtype, gene, min_seqs, \n",
    "                                             midfreq_high, midfreq_low, year_max, year_min, True)\n",
    "\n",
    "            #calculate bootstrapped rate of adaptation\n",
    "            (window_midpoints, bs_adaptive_substitutions, \n",
    "             bs_adaptive_substitutions_per_codon, \n",
    "             bs_rate_of_adaptation) = calc_bhatt_estimators(bootstrap_alignment_codons, bootstrap_outgroup_codons, \n",
    "                                                                   year_windows, bootstrap_m_ratio, midfreq_high, midfreq_low, \n",
    "                                                                   bootstrap, update_outgroup)\n",
    "\n",
    "\n",
    "            #add these bootstrap values to list\n",
    "            bootstrap_adaptive_substitutions.append(bs_adaptive_substitutions)\n",
    "            bootstrap_adaptive_substitutions_per_codon.append(bs_adaptive_substitutions_per_codon)\n",
    "            bootstrap_rates_of_adaptation.append(bs_rate_of_adaptation)\n",
    "    \n",
    "    if bootstrap:\n",
    "        return window_midpoints, adaptive_substitutions, adaptive_substitutions_per_codon, rate_of_adaptation, bootstrap_adaptive_substitutions, bootstrap_adaptive_substitutions_per_codon, bootstrap_rates_of_adaptation\n",
    "\n",
    "    else:\n",
    "        return window_midpoints, adaptive_substitutions, adaptive_substitutions_per_codon, rate_of_adaptation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "541ac416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adaptation_analysis(host, virus, subtype, genes, window, min_seqs, year_max, year_min,\n",
    "                             midfreq_high, midfreq_low, bootstrap, update_outgroup):\n",
    "    \n",
    "    data_to_plot = []\n",
    "    \n",
    "    if subtype==None:\n",
    "        virus_subtype = virus\n",
    "        virus_and_subtype = virus\n",
    "    else:\n",
    "        virus_subtype = subtype\n",
    "        virus_and_subtype = virus+'_'+subtype\n",
    "        \n",
    "        \n",
    "    for gene in genes:\n",
    "    \n",
    "        if bootstrap:\n",
    "            save_json_name = f'{virus_and_subtype}_{gene}_{window}_{min_seqs}_adaptation_bootstrapped.json'\n",
    "            save_json_folder = f\"adaptation_results/results/{host}\"\n",
    "            save_json_full_path = f\"{save_json_folder}/{save_json_name}\"\n",
    "            \n",
    "            \n",
    "            os.makedirs(save_json_folder, exist_ok=True)\n",
    "            \n",
    "            if path.exists(save_json_full_path):\n",
    "                with open(save_json_full_path) as json_handle:\n",
    "                    json_dict = json.load(json_handle)\n",
    "                    (window_midpoint, adaptive_substitutions, \n",
    "                     adaptive_substitutions_per_codon, \n",
    "                     rate_of_adaptation, bootstrap_adaptive_substitutions, \n",
    "                     bootstrap_adaptive_substitutions_per_codon, \n",
    "                     bootstrap_rate_of_adaptation) = (json_dict['window_midpoint'], \n",
    "                                                      json_dict['adaptive_substitutions'], \n",
    "                                                      json_dict['adaptive_substitutions_per_codon'], \n",
    "                                                      json_dict['rate_of_adaptation'], \n",
    "                                                      json_dict['bootstrap_adaptive_substitutions'], \n",
    "                                                      json_dict['bootstrap_adaptive_substitutions_per_codon'], \n",
    "                                                      json_dict['bootstrap_rate_of_adaptation'])\n",
    "\n",
    "            else:\n",
    "\n",
    "                (window_midpoint, adaptive_substitutions, \n",
    "                 adaptive_substitutions_per_codon, \n",
    "                 rate_of_adaptation, bootstrap_adaptive_substitutions, \n",
    "                 bootstrap_adaptive_substitutions_per_codon, \n",
    "                 bootstrap_rate_of_adaptation) = calc_bhatt_a(host, virus, subtype, gene, window, min_seqs, year_max, year_min,\n",
    "                                                                midfreq_high, midfreq_low, bootstrap, update_outgroup)\n",
    "\n",
    "                save_json = {'host':host, 'virus': virus, 'subtype':subtype, 'gene': gene, 'window':window, 'min_seqs': min_seqs, \n",
    "                             'midfreq_high': midfreq_high, 'midfreq_low': midfreq_low,\n",
    "                             'window_midpoint':window_midpoint, 'adaptive_substitutions':adaptive_substitutions, \n",
    "                             'adaptive_substitutions_per_codon':adaptive_substitutions_per_codon, 'rate_of_adaptation': rate_of_adaptation,\n",
    "                             'bootstrap_adaptive_substitutions': bootstrap_adaptive_substitutions, \n",
    "                             'bootstrap_adaptive_substitutions_per_codon': bootstrap_adaptive_substitutions_per_codon, \n",
    "                             'bootstrap_rate_of_adaptation':bootstrap_rate_of_adaptation}\n",
    "                with open(save_json_full_path, 'w') as outfile:\n",
    "                    json.dump(save_json, outfile)\n",
    "\n",
    "            slope_sci = rate_of_adaptation * (10**3)\n",
    "            bs_slope_sci = [x * (10**3) for x in bootstrap_rate_of_adaptation]\n",
    "            lower_95ci = np.percentile(sorted(bs_slope_sci), 2.5)\n",
    "            upper_95ci = np.percentile(sorted(bs_slope_sci), 97.5)\n",
    "\n",
    "            data_to_plot.append({'host':host, 'virus': virus, 'subtype': subtype, 'virus_and_subtype': virus_and_subtype, \n",
    "                                 'gene': gene,\n",
    "                                 'adaptive_subs_per_codon_per_year': slope_sci, \n",
    "                                 'lower_95ci': lower_95ci, 'upper_95ci': upper_95ci, \n",
    "                                 'ci': [lower_95ci, upper_95ci]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            save_json_file = f'{virus_and_subtype}_{gene}_{window}_{min_seqs}_adaptation.json'\n",
    "            save_json_folder = f\"adaptation_results/results/{host}\"\n",
    "            save_json_full_path = f\"{save_json_folder}/{save_json_name}\"\n",
    "                        \n",
    "            os.makedirs(save_json_folder, exist_ok=True)\n",
    "            \n",
    "            if path.exists(save_json_full_path):\n",
    "                with open(save_json_full_path) as json_handle:\n",
    "                    json_dict = json.load(json_handle)\n",
    "                    (window_midpoint, adaptive_substitutions, \n",
    "                     adaptive_substitutions_per_codon, \n",
    "                     rate_of_adaptation) = (json_dict['window_midpoint'], \n",
    "                                            json_dict['adaptive_substitutions'], \n",
    "                                            json_dict['adaptive_substitutions_per_codon'], \n",
    "                                            json_dict['rate_of_adaptation'])\n",
    "\n",
    "            else:\n",
    "                (window_midpoint, adaptive_substitutions, \n",
    "                 adaptive_substitutions_per_codon, \n",
    "                 rate_of_adaptation) = calc_bhatt_a(host, virus, subtype, gene, window, min_seqs, year_max, year_min,\n",
    "                                                     midfreq_high, midfreq_low, bootstrap, update_outgroup)\n",
    "\n",
    "\n",
    "                save_json = {'host':host, 'virus': virus, 'subtype':subtype, 'gene': gene, 'window':window, 'min_seqs': min_seqs, \n",
    "                             'midfreq_high': midfreq_high, 'midfreq_low': midfreq_low,\n",
    "                             'window_midpoint':window_midpoint, 'adaptive_substitutions':adaptive_substitutions, \n",
    "                             'adaptive_substitutions_per_codon':adaptive_substitutions_per_codon, \n",
    "                             'rate_of_adaptation': rate_of_adaptation}\n",
    "                with open(save_json_full_path, 'w') as outfile:\n",
    "                    json.dump(save_json, outfile)\n",
    "\n",
    "            slope_sci = rate_of_adaptation * (10**3)\n",
    "            data_to_plot.append({'host':host, 'virus': virus, 'subtype': subtype, 'virus_and_subtype':virus_and_subtype, \n",
    "                                 'gene': gene,\n",
    "                                 'adaptive_subs_per_codon_per_year': slope_sci})\n",
    "    \n",
    "    [print(json.dumps(element, indent=4)) for element in data_to_plot]\n",
    "    # print(data_to_plot)\n",
    "    return data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "3f13c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_genes_adaptation_rate(virus, subtype, \n",
    "                                  exclude=None,\n",
    "                                  window=5, min_seqs=3, bootstrap=False, update_outgroup=True,\n",
    "                                  midfreq_high=0.75, midfreq_low=0.15, year_max=None, year_min=None, filename=None):\n",
    "    \n",
    "\n",
    "    data_to_plot = []\n",
    "    \n",
    "    \n",
    "    configs = readin_virus_config(virus)\n",
    "    genes = configs[\"genes\"]\n",
    "    \n",
    "    if exclude!=None:\n",
    "        for e in exclude:\n",
    "            genes.remove(e)\n",
    "    \n",
    "    for gene in genes:\n",
    "        if subtype:\n",
    "            virus_and_sub = virus+'_'+subtype\n",
    "            color = configs['color'][subtype]\n",
    "            virus_name_legible = configs['legible_name'][subtype]\n",
    "\n",
    "            if gene!=\"None\":\n",
    "                data_to_plot+=run_adaptation_analysis(virus, subtype, gene, window, min_seqs, year_max, year_min,\n",
    "                             midfreq_high, midfreq_low, bootstrap, update_outgroup)\n",
    "\n",
    "        else:\n",
    "            color = configs['color']\n",
    "            virus_names_legible = configs['legible_name']\n",
    "            #np has 2 distinct clades, analyze ongoing evolution of the major/recent one\n",
    "            if virus=='vic' and gene == 'NP':\n",
    "                data_to_plot+=run_adaptation_analysis(virus, subtype, gene, window, min_seqs, year_max, 2010,\n",
    "                             midfreq_high, midfreq_low, bootstrap, update_outgroup)\n",
    "\n",
    "\n",
    "            elif gene!=\"None\":\n",
    "                data_to_plot+=run_adaptation_analysis(virus, subtype, gene, window, min_seqs, year_max, year_min,\n",
    "                             midfreq_high, midfreq_low, bootstrap, update_outgroup)\n",
    "\n",
    "    \n",
    "    df_to_plot = pd.DataFrame(data_to_plot)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, len(genes)*0.5)) \n",
    "    sns.set_style('white')\n",
    "    \n",
    "    #make y-coordinates for each gene\n",
    "    #cannot figure out how to do custom error bars in seaborn, which is why I'm doing it this way\n",
    "    y_coords = {}\n",
    "    all_y_ticks = []\n",
    "    last_coord = 0.0\n",
    "    spacing = {**{x:0.2 for x in range(1,8)}, **{y:0.3 for y in range(8,12)}}\n",
    "\n",
    "    \n",
    "    for g in genes:\n",
    "        last_coord+=spacing[len(genes)]\n",
    "        y_coords[g] = last_coord\n",
    "        all_y_ticks.append(last_coord)\n",
    "        \n",
    "    \n",
    "    for gene in genes:\n",
    "        y = y_coords[gene]\n",
    "        df_row = df_to_plot[df_to_plot['gene']==gene]\n",
    "        x = float(df_row['adaptive_subs_per_codon_per_year'])\n",
    "        if bootstrap:\n",
    "            err_lower = float(df_row['lower_95ci'])\n",
    "            err_upper = float(df_row['upper_95ci'])\n",
    "            ax.hlines(y, err_lower, err_upper, color)\n",
    "        ax.plot(x, y, 'o', ms=12, color=color)\n",
    "        \n",
    "        ax.tick_params(axis='y',size=0)\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    plt.xlabel(r'Adaptive Muts per Codon per Year $(x 10^{-3})$', size=16)\n",
    "            \n",
    "    plt.rcParams['xtick.labelsize']=14\n",
    "\n",
    "    # label points according to gene name        \n",
    "    ax.set_yticks(list(y_coords.values()))\n",
    "        \n",
    "    ax.set_yticklabels(list(y_coords.keys()), size=12)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # remove box around plot\n",
    "    sns.despine(left=False, bottom=False)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    if filename:\n",
    "        fig.savefig(filename, dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "99ec2b6d-dc7e-4424-82a9-8f3dda64884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/monclalab1/Documents/nonhuman_H3_project/non-human-h3/host_specific/04-2023_11-2023/augur_aligned/h3n2/results/aligned_h3nx_cladeNA2012_ha.fasta\n",
      "/Users/monclalab1/Documents/nonhuman_H3_project/non-human-h3/host_specific/04-2023_11-2023/augur_aligned/h3n2/results/aligned_h3nx_cladeNA2012_ha.fasta\n",
      "h3nx ha: 0 bootstraps done\n",
      "h3nx ha: 10 bootstraps done\n",
      "h3nx ha: 20 bootstraps done\n",
      "h3nx ha: 30 bootstraps done\n",
      "h3nx ha: 40 bootstraps done\n",
      "h3nx ha: 50 bootstraps done\n",
      "h3nx ha: 60 bootstraps done\n",
      "h3nx ha: 70 bootstraps done\n",
      "h3nx ha: 80 bootstraps done\n",
      "h3nx ha: 90 bootstraps done\n",
      "{\n",
      "    \"host\": \"NA2012 Swine\",\n",
      "    \"virus\": \"h3nx\",\n",
      "    \"subtype\": null,\n",
      "    \"virus_and_subtype\": \"h3nx\",\n",
      "    \"gene\": \"ha\",\n",
      "    \"adaptive_subs_per_codon_per_year\": 1.5784173475399703,\n",
      "    \"lower_95ci\": 0.49988152860242585,\n",
      "    \"upper_95ci\": 2.8143133611887903,\n",
      "    \"ci\": [\n",
      "        0.49988152860242585,\n",
      "        2.8143133611887903\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "warnings.simplefilter('ignore', BiopythonWarning)\n",
    "\n",
    "readin_virus_config(\"h3nx\")\n",
    "\n",
    "#genes = [\"ha\", \"pb1\", \"na\", \"pb2\" , \"mp\", \"np\", \"ns\", \"pa\"]\n",
    "genes = [\"ha\"]\n",
    "\n",
    "data = run_adaptation_analysis(\n",
    "    \"NA2012 Swine\",\n",
    "    \"h3nx\", \n",
    "    subtype= None, \n",
    "    genes= genes,                           \n",
    "    window=3, \n",
    "    min_seqs=3, \n",
    "    bootstrap=True, \n",
    "    update_outgroup=True,\n",
    "    midfreq_high=0.75, \n",
    "    midfreq_low=0.15, \n",
    "    year_max=None, \n",
    "    year_min=None\n",
    ")\n",
    "# df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207982f-49c4-4eb9-819d-8d28160dce0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b429f9-c891-4c0b-a316-9dd0969290ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
