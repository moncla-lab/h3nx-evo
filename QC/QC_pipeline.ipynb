{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6c130a3c-3cf7-4bb7-aaa3-47806a2db4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "module_dir = \"scripts\" #change so it is the correct path\n",
    "sys.path.append(module_dir)\n",
    "\n",
    "from fasta_editing import fasta_to_df, fasta_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26592f68-7dac-4b73-9bd0-5e8bd0f455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this uses the master fasta file to parse out sequences by segment\n",
    "#QC is done here but region and species data is done in the NCBI_QC function\n",
    "\n",
    "def ncbi_prep(fasta_file, gene_segment_map, parsed_output):\n",
    "    \n",
    "    df = fasta_to_df(fasta_file)\n",
    "    \n",
    "    df['header'] = df['header'].str.replace(' ', '_')\n",
    "    df['Strain'] = df['header'].str.split(\"|\").str[0]\n",
    "    df['Accession'] = df['header'].str.split(\"|\").str[1]\n",
    "    df['Subtype'] = df['header'].str.split(\"|\").str[2]\n",
    "    df['Date'] = df['header'].str.split(\"|\").str[3]\n",
    "    df['Host'] = df['header'].str.split(\"|\").str[4]\n",
    "    df['Country'] = df['header'].str.split(\"|\").str[5]\n",
    "    df['Segment'] = df['header'].str.split(\"|\").str[6]\n",
    "\n",
    "    #QC steps specific to NCBI\n",
    "    df.Accession = df.Accession.str[:-2]\n",
    "    df['Strain'] = df['Strain'].str.replace('>Influenza_A_virus_', '', regex=False)\n",
    "    df['Strain'] = df['Strain'].str.extract(r'(\\(.*?\\))')\n",
    "    df['Strain'] = df['Strain'].str.replace('^\\(', '>', regex=True)\n",
    "    df['Strain'] = df['Strain'].str.replace('\\(\\w+\\)', '', regex=True)\n",
    "    \n",
    "    df = df[df[\"Date\"] != \"--\"]\n",
    "    df[\"Date\"] = df[\"Date\"].str.replace('/', '-')\n",
    "    # print(df.Host.unique())\n",
    "    \n",
    "    df['header'] = df[['Strain', 'Accession', 'Subtype', 'Date', 'Host', 'Country']].apply('|'.join, axis=1)\n",
    "    \n",
    "    for segment, gene in gene_segment_map.items():\n",
    "        segment_df = df[df['Segment'] == segment]\n",
    "        fasta_writer(parsed_output, f\"h3nx_{gene}.fasta\", segment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e086dd-cd88-4b09-90e5-3de13cbdbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this uses the master fasta file to parse out sequences by segment\n",
    "#QC is done here but region and species data is done in the NCBI_QC function\n",
    "\n",
    "def gisaid_prep(fasta_file, gene_segment_map, parsed_output):\n",
    "    \n",
    "    df = fasta_to_df(fasta_file)\n",
    "    \n",
    "    df['header'] = df['header'].str.replace(' ', '_')\n",
    "    df['Strain'] = df['header'].str.split(\"|\").str[0]\n",
    "    df['Accession'] = df['header'].str.split(\"|\").str[1]\n",
    "    df['Date'] = df['header'].str.split(\"|\").str[2]\n",
    "    df['Segment'] = df['header'].str.split(\"|\").str[3]\n",
    "    df['Species'] = df['Strain'].str.split(\"/\").str[1]\n",
    "    df = df[~df[\"Species\"].str.lower().str.contains(\"environment|mouse|ferret\")]\n",
    "    \n",
    "    df = df[df[\"Date\"] != \"--\"]\n",
    "    df[\"Date\"] = df[\"Date\"].str.replace('/', '-')\n",
    "\n",
    "    df['header'] = df[['Strain', 'Accession', 'Date', 'Species']].apply('|'.join, axis=1)\n",
    "    \n",
    "    for segment, gene in gene_segment_map.items():\n",
    "        segment_df = df[df['Segment'] == segment]\n",
    "        fasta_writer(parsed_output, f\"h3nx_{gene}.fasta\", segment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81f22a6f-7a63-48d0-aeea-3bb8d07a0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function should be run to finish the QC needed for nextstrain (QC folder)\n",
    "#it also will dedupe the sequences (dedupe folder) and can standardize dates if needed (consistent)\n",
    "\n",
    "def ncbi_qc(list_of_genes, input_path, output_path, species_csv, \n",
    "            regions_csv, dedupe = True):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    \n",
    "    for gene in genes:\n",
    "        \n",
    "        df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "\n",
    "        #this can be customized, change it based on how the headers are in your data\n",
    "        df['header'] = df['header'].str.replace(' ', '_')\n",
    "        df['Strain'] = df['header'].str.split(\"|\").str[0]\n",
    "        df['Accession'] = df['header'].str.split(\"|\").str[1]\n",
    "        df['Subtype'] = df['header'].str.split(\"|\").str[2]\n",
    "        df['Date'] = df['header'].str.split(\"|\").str[3]\n",
    "        df['Country'] = df['header'].str.split(\"|\").str[5]\n",
    "        df['Species'] = df['Strain'].str.split(\"/\").str[1]\n",
    "        \n",
    "        #cleanup based on previous problems with data\n",
    "        df.Country.replace('Viet_Nam', 'Vietnam' , inplace =True)    \n",
    "        df = df[df[\"Country\"] != \"\"]\n",
    "        df = df[~df[\"Subtype\"].str.contains(\"H3Nx|H3,mixed|mixed,H3|mixed,_H3|Mixed,H3|mixed.H3|H3N-\")]\n",
    "        df.Subtype.replace('H3N6,H3', 'H3N6', inplace =True)\n",
    "        df = df[~df[\"Species\"].str.lower().str.contains(\"animal|environment|ferret|mouse\")]\n",
    "        \n",
    "        #adding Xs whereever dates are incomplete\n",
    "        df['Date'] = df['Date'].str.replace(r'--', '-XX-XX', regex=True)\n",
    "        df['Date'] = df['Date'].str.replace(r'-$', '-XX', regex=True)\n",
    "        df['Date'] = df['Date'].str.replace(r\"^(\\d{4}-\\d{2})$\" , r\"\\1-XX\", regex=True)\n",
    "        df['Date'] = df['Date'].str.replace(r\"^(\\d{4})$\" , r\"\\1-XX-XX\", regex=True)\n",
    "        \n",
    "        #adding region data\n",
    "        regions = pd.read_csv(regions_csv)\n",
    "        df = df.merge(regions,left_on = df[\"Country\"].str.lower(), right_on= regions[\"country\"], how= \"left\")\n",
    "        df.drop(['key_0'], axis=1, inplace =True)\n",
    "        #NCBI for some reason has latin names for host while genbank had \"canine\" or \"swine\"\n",
    "        #standardizing that here\n",
    "\n",
    "        df = speciesClean(df, species_csv)\n",
    "        \n",
    "        df['header'] = df[['Strain', 'Accession', 'Subtype', 'Date', 'Host', 'country', 'region','Species', 'Broad', 'Order']].apply('|'.join, axis=1)\n",
    "        \n",
    "        fasta_writer(f\"{output_path}QC/\", f\"h3nx_{gene}.fasta\", df)\n",
    "        \n",
    "    if dedupe:\n",
    "        fastaDeDupe(list_of_genes, f\"{output_path}QC/\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e68ea372-58e1-45a4-91ae-cd6aa274be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function should be run to finish the QC needed for nextstrain (QC folder)\n",
    "#it also will dedupe the sequences (dedupe folder) and can standardize dates if needed (consistent)\n",
    "def gisaid_qc(list_of_genes, metadata_path, input_path, output_path, species_csv, \n",
    "              regions_csv,  dedupe = True):\n",
    "    \n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "    #replacing any spaces in the Isolate_Name column with underscores\n",
    "    #adding the > character so that you can find matches in the .fasta file\n",
    "    metadata['Isolate_Name'] = metadata['Isolate_Name'].str.replace(' ', '_')\n",
    "    metadata['Isolate_Name'] = '>' + metadata['Isolate_Name'].astype(str)\n",
    "\n",
    "    #extracting the country name as the second value in the location column\n",
    "    #location is formatted continent/country/state/county)\n",
    "    #drops any sequences where location or country data is not available\n",
    "    metadata.dropna(subset=['Location'], inplace=True)\n",
    "    metadata['Country'] = metadata['Location'].str.split('/').str[1].str.strip()\n",
    "    metadata.dropna(subset=['Country'], inplace=True)\n",
    "\n",
    "    genes = list_of_genes\n",
    "    \n",
    "    for gene in genes:\n",
    "        \n",
    "        df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "\n",
    "        #make sure this matches your data\n",
    "        df['Strain'] = df['header'].str.split(\"|\").str[0]\n",
    "        df['Accession'] = df['header'].str.split(\"|\").str[1]\n",
    "        df['Date'] = df['header'].str.split(\"|\").str[2]\n",
    "        df['Species'] = df['header'].str.split(\"|\").str[3]\n",
    "        \n",
    "        #adding Xs whereever dates are incomplete\n",
    "        df['Date'] = df['Date'].str.replace(r'--', '-XX-XX', regex=True)\n",
    "        df['Date'] = df['Date'].str.replace(r'-$', '-XX', regex=True)\n",
    "        df['Date'] = df['Date'].str.replace(r\"^(\\d{4}-\\d{2})$\" , r\"\\1-XX\", regex=True)\n",
    "        df['Date'] = df['Date'].str.replace(r\"^(\\d{4})$\" , r\"\\1-XX-XX\", regex=True)\n",
    "\n",
    "        #merging metadata with df on Isolate_Name column, adding metadata columns youre interested in\n",
    "        merged = pd.merge(df, metadata[['Isolate_Name', 'Subtype', 'Country']], left_on='Strain', right_on='Isolate_Name')\n",
    "        \n",
    "        #country + host QC and replacing spaces\n",
    "        merged.Country.replace('United States', 'USA', inplace =True)\n",
    "        merged.Country.replace('Korea, Republic of', 'South Korea' , inplace =True)\n",
    "        merged.Country.replace('Russian Federation', 'Russia' , inplace =True)\n",
    "        merged.Country.replace('Hong Kong (SAR)', 'Hong Kong', inplace =True)\n",
    "        merged.Country.replace(\"Lao, People's Democratic Republic\", \"Laos\", inplace =True)\n",
    "        merged.Country = merged.Country.str.replace(' ', '_')\n",
    "        merged['Subtype'] = merged['Subtype'].str.replace('A / ', '', regex=False)\n",
    "        \n",
    "        #adding region data\n",
    "        regions = pd.read_csv(regions_csv)\n",
    "        merged = merged.merge(regions,left_on = merged[\"Country\"].str.lower(), right_on= regions[\"country\"], how= \"left\")\n",
    "\n",
    "        merged.drop(['key_0'], axis=1, inplace =True)\n",
    "        \n",
    "        merged = speciesClean(merged, species_csv)\n",
    "        \n",
    "        #the fields are in the same order as in the ncbi QC, just named differently\n",
    "        merged['header'] = merged[['Strain', 'Accession', 'Subtype', 'Date', 'Host', 'Country', 'region', 'Species', 'Broad','Order']].apply('|'.join, axis=1)\n",
    "        \n",
    "        fasta_writer(f\"{output_path}QC/\", f\"h3nx_{gene}.fasta\", merged) \n",
    "\n",
    "    if dedupe:\n",
    "        fastaDeDupe(list_of_genes, f\"{output_path}QC/\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "58085b01-a867-40ef-a298-c7ce609084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the purpose of this function is to remove duplicates by keeping the longest sequence\n",
    "#while also keeping the most complete date\n",
    "#this is because different researchers sometimes upload the same strain but the date \n",
    "#will be incomplete in one vs the other\n",
    "\n",
    "#by default, it also calls standardize_dates which makes sure all strains ACROSS fasta files\n",
    "#have the most complete date associated with each strain\n",
    "\n",
    "def fastaDeDupe(list_of_genes, input_path, output_path):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "\n",
    "    for gene in genes: #make sure your file names formatted as h3nx_[gene}.fasta\n",
    "        \n",
    "        df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "        \n",
    "        df['strain'] = df['header'].str.split(\"|\").str[0].str.lower()\n",
    "        df['date'] = df['header'].str.split(\"|\").str[3]\n",
    "\n",
    "        # to double check dupes were taken out\n",
    "        # duplicated_strains = df[df.duplicated(subset=\"strain\")][['strain','sequence']]\n",
    "        # duplicated_strains.strain.unique()\n",
    "        \n",
    "        # group by sequence length and date completeness, it keeps the longest sequence and the \n",
    "        # most complete date\n",
    "        temp_df = df.groupby(['strain']).agg({\n",
    "            \"sequence\": lambda s: max(s, key=len),\n",
    "            \"date\": lambda s: max(s.str.replace(\"XX\", \"\"), key=len)\n",
    "        })\n",
    "        \n",
    "        new_df = temp_df.merge(right=df, on=[\"strain\", \"sequence\"], how=\"inner\", suffixes=[\"\", \"_OLD\"])\n",
    "         \n",
    "        new_df[\"header\"]= new_df.apply(lambda x: x['header'].replace(str(x['date_OLD']), str(x['date'])), axis=1)\n",
    "        \n",
    "        new_df = new_df.loc[:,~new_df.columns.str.endswith('_OLD')]\n",
    "        \n",
    "        #if the date and sequence are the same between duplicates, it wont drop it above\n",
    "        #this line will make sure ALL duplicates are finally dropped\n",
    "        new_df.drop_duplicates(subset=['strain'], keep='first', inplace=True, ignore_index=True)\n",
    "        \n",
    "        fasta_writer(f\"{output_path}/deduped/\", f\"h3nx_{gene}.fasta\", new_df)\n",
    "        \n",
    "        standardize_dates(list_of_genes, f\"{output_path}/deduped/\", f\"{output_path}/consistent/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54adf2ef-7c87-4de7-ac80-e6da1c607eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dates(list_of_genes,input_path, output_path):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for gene in list_of_genes:\n",
    "        gene_df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "        gene_df[\"gene\"] = gene\n",
    "        df = pd.concat([df, gene_df], ignore_index=True)\n",
    "\n",
    "    df['strain'] = df['header'].str.split(\"|\").str[0].str.lower()\n",
    "    df['date'] = df['header'].str.split(\"|\").str[3].str.replace(\"XX\", \"\")\n",
    "    df['dateX'] = df['header'].str.split(\"|\").str[3]\n",
    "\n",
    "    df['new_date'] = df.groupby('strain')['date'].transform('max')\n",
    "\n",
    "    df['new_date'] = df['new_date'].str.replace(r'--', '-XX-XX', regex=True)\n",
    "    df['new_date'] = df['new_date'].str.replace(r'-$', '-XX', regex=True)\n",
    "\n",
    "    df['header'] =  df.apply(lambda x: x['header'].replace(str(x['dateX']), str(x[\"new_date\"])), axis=1)\n",
    "\n",
    "    for gene in list_of_genes:\n",
    "        gene_df = df[df['gene'] == gene]\n",
    "        fasta_writer(output_path, f\"h3nx_{gene}.fasta\", gene_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1331a947-017b-4149-9e92-836e9d3b1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speciesClean(df, species_csv):\n",
    "    \n",
    "    species_df = pd.read_csv(species_csv)\n",
    "    \n",
    "    species_df['annotated_lower'] = species_df['annotated'].str.lower()\n",
    "    species_df['correction_lower'] = species_df['correction'].str.lower()\n",
    "    df['Species_lower'] = df['Species'].str.lower()\n",
    "    \n",
    "    merged_df = df.merge(species_df, \n",
    "                         left_on='Species_lower', \n",
    "                         right_on='annotated_lower', \n",
    "                         how='left')\n",
    "    \n",
    "    # this will raise an error if there are species in the passed df that are not found in species_df.annotated.\n",
    "    missing_species = merged_df.loc[merged_df['host'].isnull(), 'Species'].unique()\n",
    "    if len(missing_species) > 0:\n",
    "        raise ValueError(f\"You need to update species.csv to include: {missing_species}\")\n",
    "    \n",
    "    merged_df['Species'] = merged_df['correction'].fillna(merged_df['Species']).str.lower()\n",
    "    \n",
    "    merged_df['Host'] = merged_df['host']\n",
    "    merged_df['Broad'] = merged_df['broad']\n",
    "    merged_df['Order'] = merged_df['order']\n",
    "    \n",
    "    result_df = merged_df.drop(['Species_lower', 'annotated_lower', 'correction_lower', \n",
    "                                'correction', 'host', 'broad', 'order'], axis=1)\n",
    "    \n",
    "    result_df.drop_duplicates(subset=['Strain'], keep='first', inplace=True, ignore_index=True)\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4897e342-d231-4fa5-a6f3-59e76784af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes in your gisaid and you genbank data, assuming all QC has been done on both, and appends \n",
    "#the gisaid data to the genbank, then calls the deDupe function \n",
    "def merge(list_of_genes, gisaid_path, NCBI_path, merged_path, dedupe=True):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    \n",
    "    try:  \n",
    "        os.mkdir(merged_path)\n",
    "\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "    for gene in genes:\n",
    "        with open(f\"{NCBI_path}h3nx_{gene}.fasta\" , 'r') as f2, open(f\"{gisaid_path}h3nx_{gene}.fasta\", 'r') as f1, open(f\"{merged_path}h3nx_{gene}.fasta\", 'w') as f3:\n",
    "            f3.write(f2.read())\n",
    "            f3.write(f1.read())\n",
    "\n",
    "    if dedupe:\n",
    "        fastaDeDupe(list_of_genes, merged_path, merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01660494-8098-4a3d-b17c-5695b89a8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes in your gisaid and you genbank data, assuming all QC has been done on both, and appends \n",
    "#the gisaid data to the genbank, then calls the deDupe function \n",
    "def mergeWithMA(list_of_genes, current_sequence_path, current_sequences_path, merged_path, dedupe=True):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    \n",
    "    try:  \n",
    "        os.mkdir(merged_path)\n",
    "\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "    for gene in genes:\n",
    "        with open(f\"{current_sequence_path}h3nx_{gene}.fasta\" , 'r') as f2, open(f\"{current_sequences_path}h3nx_{gene}.fasta\", 'r') as f1, open(f\"{merged_path}h3nx_{gene}.fasta\" , 'a+') as f3:\n",
    "            f3.write(f2.read())\n",
    "            f3.write(f1.read())\n",
    "\n",
    "    if dedupe:\n",
    "        fastaDeDupe(list_of_genes, merged_path, merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1b84bb92-f380-4dec-95f8-474b25d8deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first need to download the data from ncbi and gisaid separately \n",
    "\n",
    "def wrapper_func(list_of_genes, gene_segment_map, data_main_folder, species_csv, regions_csv,\n",
    "                 ncbi = False, ncbi_data_path = None, ncbi_data_filename = None, \n",
    "                 gisaid = False, gisaid_data_path = None, gisaid_data_filename = None, \n",
    "                 gisaid_metadata_filename = None, master_merge = False, master_data_path = None\n",
    "                ):\n",
    "\n",
    "\n",
    "    if ncbi and (ncbi_data_path is None or ncbi_data_filename is None):\n",
    "        raise ValueError(\"NCBI paths must be specified when 'ncbi' is True.\")\n",
    "    if gisaid and (gisaid_data_path is None or gisaid_data_filename is None or gisaid_metadata_filename is None):\n",
    "        raise ValueError(\"GISAID paths must be specified when 'gisaid' is True.\")\n",
    "    if master_merge and master_data_path is None:\n",
    "        raise ValueError(\"Master merge path must be specified when 'master_merge' is True.\")\n",
    "\n",
    "\n",
    "    if ncbi:\n",
    "        \n",
    "        ncbi_prep(f\"{data_main_folder}{ncbi_data_path}{ncbi_data_filename}\", \n",
    "                  gene_segment_map, \n",
    "                  f\"{data_main_folder}{ncbi_data_path}parsed/\"\n",
    "                 )\n",
    "        \n",
    "        ncbi_qc(list_of_genes, \n",
    "                f\"{data_main_folder}{ncbi_data_path}parsed/\", \n",
    "                f\"{data_main_folder}{ncbi_data_path}\", \n",
    "                species_csv,\n",
    "                regions_csv,\n",
    "                dedupe = True,\n",
    "               )\n",
    "\n",
    "    if gisaid:\n",
    "        \n",
    "        gisaid_prep(f\"{data_main_folder}{gisaid_data_path}{gisaid_data_filename}\", \n",
    "                    gene_segment_map, \n",
    "                    f\"{data_main_folder}{gisaid_data_path}parsed/\"\n",
    "                   ) \n",
    "        \n",
    "        gisaid_qc(list_of_genes, \n",
    "                  f\"{data_main_folder}{gisaid_data_path}{gisaid_metadata_filename}\", \n",
    "                  f\"{data_main_folder}{gisaid_data_path}parsed/\", \n",
    "                  f\"{data_main_folder}{gisaid_data_path}\",\n",
    "                  species_csv,\n",
    "                  regions_csv,\n",
    "                  dedupe = True\n",
    "                 )\n",
    "        \n",
    "    if gisaid and ncbi:\n",
    "        \n",
    "        merge(list_of_genes,\n",
    "              f\"{data_main_folder}{gisaid_data_path}consistent/\", \n",
    "              f\"{data_main_folder}{ncbi_data_path}consistent/\",\n",
    "              f\"{data_main_folder}merged/\"\n",
    "             )\n",
    "\n",
    "    # need to make standardize more efficient becuase it doesnt work on big fasta files\n",
    "    # since it iterates over every unique strain\n",
    "    if master_merge:\n",
    "        \n",
    "        mergeWithMA(list_of_genes,\n",
    "                    master_data_path, \n",
    "                    f\"{data_main_folder}merged/consistent/\",\n",
    "                    \"./master_merged/\",\n",
    "                    dedupe = True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "56a268df-c9f8-4d89-b086-6bcc6162f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_genes = [\"ha\", \"pb1\", \"pb2\",\"pa\",\"mp\",\"np\",\"na\",\"ns\"]\n",
    "\n",
    "gene_segment_map = {\n",
    "    \"1\" : \"pb2\",\n",
    "    \"2\" : \"pb1\",\n",
    "    \"3\" : \"pa\",\n",
    "    \"4\" : \"ha\",\n",
    "    \"5\" : \"np\",\n",
    "    \"6\" : \"na\",\n",
    "    \"7\" : \"mp\",\n",
    "    \"8\" : \"ns\"\n",
    "}\n",
    "\n",
    "# before you run this, make sure you have downloaded sequences from either gisaid or ncbi\n",
    "# the data pulls must have a segment number as the last field so it can be parsed into segment\n",
    "# specific fasta files.\n",
    "# qc_pipeline should be outside of a main folder (data_pulls for example) which then has an ncbi and/or \n",
    "# gisaid subfolder as well\n",
    "# also save the .xls of gisaid metadata as a .csv for the gisaid QC steps\n",
    "\n",
    "\n",
    "wrapper_func(list_of_genes, \n",
    "             gene_segment_map, \n",
    "             data_main_folder = \"./data_pulls/\", \n",
    "             species_csv = \"./species.csv\",\n",
    "             regions_csv = \"./regions.csv\",\n",
    "             ncbi = True, \n",
    "             ncbi_data_path = \"ncbi/\", \n",
    "             ncbi_data_filename = \"all_ncbi.fasta\", \n",
    "             gisaid = True, \n",
    "             gisaid_data_path = \"gisaid/\", \n",
    "             gisaid_data_filename = \"all_gisaid.fasta\", \n",
    "             gisaid_metadata_filename = \"all_gisaid.csv\", \n",
    "             master_merge = True,\n",
    "             master_data_path = \"./new_master/\",\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
